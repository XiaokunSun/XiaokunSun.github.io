---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# ü´† About Me 
I am currently a Ph.D. student (2024.09 - Now), working in <a href='https://is.nju.edu.cn/main.htm'> School of Intelligence Science and Technology, Nanjing University </a>, with <a href='https://jessezhang92.github.io/'> Zhenyu Zhang (Âº†ÊåØÂÆá) </a>. 
My research focuses on human-centric 3D digitization, including 3D human representation, reconstruction, generation and animation, etc.

I graduated from <a href='https://ci.hfut.edu.cn/index.htm'> School of Computer Science and Information Engineering, Hefei University of Technology </a> with a bachelor‚Äôs degree and from <a href='https://cic.tju.edu.cn/'> College of Intelligence and Computing, Tianjin University </a> with a master‚Äôs degree, advised by <a href='https://cic.tju.edu.cn/faculty/likun/index.html'> Kun Li (ÊùéÂù§) </a>, working closely with <a href='https://users.cs.cf.ac.uk/Yukun.Lai/'> Yu-Kun Lai (Êù•ÁÖúÂù§) </a> and <a href='https://seea.tju.edu.cn/info/1015/1608.htm'> Jingyu Yang (Êù®Êï¨Èí∞) </a>.

# üî• News
- *2024.09*: &nbsp;üéâ My Ph.D. student life begins, please wish me luck!

# üìù Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/Barbie.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Barbie: Text to Barbie-Style 3D Avatars](https://arxiv.org/pdf/2408.09126)

**Xiaokun Sun**, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang 

[**Project**](https://xiaokunsun.github.io/Barbie.github.io/) | [**Paper**](https://arxiv.org/pdf/2408.09126) | [**Code**](https://github.com/XiaokunSun/Barbie) | [**Gallery**](https://drive.google.com/drive/folders/1FXDROWXrnsSQiOZ4vBgA_Yzib3irLNBc)
<!-- - Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/SemanticHuman.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.html)

**Xiaokun Sun**, Qiao Feng, Xiongzheng Li, Jinsong Zhang, Yu-Kun Lai, Jingyu Yang, Kun Li

[**Project**](https://cic.tju.edu.cn/faculty/likun/projects/SemanticHuman/index.html) | [**Paper**](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.html) | [**Code**](https://github.com/XiaokunSun/SemanticHuman)
<!-- - Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. -->

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TVCG 2022</div><img src='images/InnerBody.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning to Infer Inner-Body under Clothing from Monocular Video](https://ieeexplore.ieee.org/document/9869633)

Xiongzheng Li*, Jing Huang*, Jinsong Zhang, **Xiaokun Sun**, Haibiao Xuan, Yu-Kun Lai, Yingdi Xie, Jingyu Yang, Kun Li (*=Equal contribution)

[**Project**](https://cic.tju.edu.cn/faculty/likun/projects/Inner-Body/index.html) | [**Paper**](https://ieeexplore.ieee.org/document/9869633) | [**Code**](https://github.com/xz-pisces/Inner-body) | [**Dataset**](https://ieee-dataport.org/documents/inner-body-dataset)
<!-- - Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. -->

</div>
</div>

# üì¶Ô∏è Open-Source Repositories
[Barbie](https://github.com/XiaokunSun/Barbie)

[![](https://img.shields.io/github/stars/XiaokunSun/Barbie)](https://github.com/XiaokunSun/Barbie)
[![](https://img.shields.io/github/forks/XiaokunSun/Barbie)](https://github.com/XiaokunSun/Barbie)
[![](https://img.shields.io/github/issues/XiaokunSun/Barbie)](https://github.com/XiaokunSun/Barbie)

---

[SemanticHuman](https://github.com/XiaokunSun/SemanticHuman)

[![](https://img.shields.io/github/stars/XiaokunSun/SemanticHuman)](https://github.com/XiaokunSun/SemanticHuman)
[![](https://img.shields.io/github/forks/XiaokunSun/SemanticHuman)](https://github.com/XiaokunSun/SemanticHuman)
[![](https://img.shields.io/github/issues/XiaokunSun/SemanticHuman)](https://github.com/XiaokunSun/SemanticHuman)

# üìñ Educations
- *2024.09 - now*, PhD, Nanjing University, Suzhou, China.
- *2021.09 - 2024.01*, Master, Tianjing University, Tianjin, China.
- *2017.09 - 2021.06*, Bachelor, Hefei University of Technology, Hefei, China.

# üíª Internships
- *2023.04 - 2023.10*, <a href='https://damo.alibaba.com/labs/xr-lab'> XR Lab at Alibaba </a>, Beijing, China.

# üèÜ Honors and Awards
- *2019.10* National First Prize in <a href='https://en.mcm.edu.cn/'> Contemporary Undergraduate Mathematical Contest in Modeling </a> (Top 0.75%)

